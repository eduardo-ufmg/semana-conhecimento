{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log, pi\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils.validation import check_array, check_X_y, check_is_fitted\n",
    "\n",
    "class RBFKNeighborsClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    k-NN classifier with RBF (isotropic multivariate normal) weighting.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int, default=sqrt(n_training_samples)\n",
    "        Number of neighbors to consider.\n",
    "    h : float, default=\\frac{e^{-1}}{mean(dist_training_samples)^{-2}+mean(dist_training_samples)^{-1/2}}\n",
    "        Radius (std. deviation) of the isotropic RBF (covariance = h^2 * I).\n",
    "    \"\"\"\n",
    "\n",
    "    def default_k(self, n_samples):\n",
    "        return int(np.floor(np.sqrt(n_samples)))\n",
    "\n",
    "    def default_h(self, n_samples, X):\n",
    "        # compute average pairwise distance (inefficient for large n_samples)\n",
    "        dists = np.linalg.norm(X[:, np.newaxis, :] - X[np.newaxis, :, :], axis=2)\n",
    "        mean_dist = np.sum(dists) / (n_samples * (n_samples - 1))  # exclude self-distances\n",
    "        return np.exp(-1) / (mean_dist ** -2 + mean_dist ** -0.5)\n",
    "\n",
    "    def __init__(self, k: int | None = None, h: float | None = None):\n",
    "        self.k_ = k\n",
    "        self.h_ = h\n",
    "\n",
    "    def pre_fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, ensure_min_samples=1, dtype=float)\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = self.pre_fit(X, y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # default k: sqrt(n_samples)\n",
    "        if self.k_ is None:\n",
    "            self.k_ = self.default_k(n_samples)\n",
    "\n",
    "        # default h: based on average pairwise distance in training set\n",
    "        if self.h_ is None:\n",
    "            self.h_ = self.default_h(n_samples, X)\n",
    "\n",
    "\n",
    "        if self.h_ <= 0:\n",
    "            raise ValueError(\"h must be > 0 (standard deviation of RBF).\")\n",
    "        \n",
    "        k_eff = min(max(1, self.k_), n_samples)\n",
    "\n",
    "        # store data\n",
    "        self.X_ = X.copy()\n",
    "        self.y_ = np.asarray(y).copy()\n",
    "        self.n_features_in_ = n_features\n",
    "\n",
    "        # classes and integer-encoded labels\n",
    "        self.classes_, inv = np.unique(self.y_, return_inverse=True)\n",
    "        self._y_encoded = inv  # length n_samples, values in [0, n_classes-1]\n",
    "        self.n_classes_ = self.classes_.shape[0]\n",
    "\n",
    "        # nearest-neighbor structure\n",
    "        self._k_eff = k_eff\n",
    "        self._nn = NearestNeighbors(n_neighbors=self._k_eff)\n",
    "        self._nn.fit(self.X_)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _rbf_logpdf_const(self):\n",
    "        # constant term of isotropic multivariate normal log-pdf\n",
    "\n",
    "        if self.k_ is None or self.h_ is None:\n",
    "            raise ValueError(\"k and h must be set (not None) before calling this method.\")\n",
    "\n",
    "        d = self.n_features_in_\n",
    "        return -0.5 * d * (np.log(2 * pi) + np.log(self.h_ * self.h_))\n",
    "    \n",
    "    def kernel(self, X):\n",
    "        check_is_fitted(self, attributes=['X_', 'y_', '_nn', 'k_', 'h_'])\n",
    "        X = check_array(X, dtype=float)\n",
    "        n_test = X.shape[0]\n",
    "\n",
    "        if self.k_ is None or self.h_ is None:\n",
    "            raise ValueError(\"k and h must be set (not None) before calling this method.\")\n",
    "\n",
    "        # get k nearest neighbors (distances returned in Euclidean norm)\n",
    "        dists, idxs = self._nn.kneighbors(X, n_neighbors=self._k_eff, return_distance=True)\n",
    "        # dists shape (n_test, k), idxs same shape\n",
    "        dists_sq = dists ** 2  # squared euclidean distances\n",
    "\n",
    "        # compute log-weights for each neighbor: logpdf of N(mu=neighbor, cov=h^2 I)\n",
    "        const = self._rbf_logpdf_const()\n",
    "        logw = const - 0.5 * (dists_sq / (self.h_ * self.h_))  # shape (n_test, k)\n",
    "\n",
    "        # numerical stabilization: subtract max per row before exponentiation\n",
    "        max_logw = np.max(logw, axis=1, keepdims=True)\n",
    "        stable_w = np.exp(logw - max_logw)  # now safe to exponentiate\n",
    "\n",
    "        # accumulate weighted votes per class\n",
    "        probs = np.zeros((n_test, self.n_classes_), dtype=float)\n",
    "        neighbor_classes = self._y_encoded[idxs]  # shape (n_test, k)\n",
    "\n",
    "        # vectorized accumulation per sample using bincount (loop over tests)\n",
    "        for i in range(n_test):\n",
    "            w = stable_w[i]\n",
    "            cls_idx = neighbor_classes[i]\n",
    "            # sum weights for each class among neighbors\n",
    "            counts = np.bincount(cls_idx, weights=w, minlength=self.n_classes_)\n",
    "            probs[i, :] = counts\n",
    "\n",
    "        return probs, neighbor_classes\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        probs, neighbor_classes = self.kernel(X)\n",
    "\n",
    "        # normalize to probabilities\n",
    "        row_sums = probs.sum(axis=1, keepdims=True)\n",
    "        # if any row sums are zero (unlikely due to stabilization), assign uniform over neighbor classes\n",
    "        zero_mask = (row_sums.squeeze() == 0)\n",
    "        if zero_mask.any():\n",
    "            for i in np.nonzero(zero_mask)[0]:\n",
    "                present = np.unique(neighbor_classes[i])\n",
    "                probs[i, :] = 0.0\n",
    "                probs[i, present] = 1.0 / present.size\n",
    "            row_sums = probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        probs /= row_sums\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes_[idx]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "    \n",
    "    def likelihood_space(self):\n",
    "        \"\"\"\n",
    "        Returns the training data projection in the likelihood space.\n",
    "        \"\"\"\n",
    "        likelihoods, _ = self.kernel(self.X_)\n",
    "\n",
    "        # Normalize by the max likelihood to ensure the domain is [0, 1]\n",
    "        likelihoods /= np.max(likelihoods)\n",
    "\n",
    "        return likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c14145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0) #type: ignore\n",
    "clf_ref = RBFKNeighborsClassifier()\n",
    "clf_ref.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", clf_ref.score(X_test, y_test))\n",
    "probs = clf_ref.predict_proba(X_test[:5])\n",
    "print(\"Probs for first 5 test samples:\\n\", probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_spirals(n_samples=100, turns=3, noise=0.1, dr=0.1, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples_per_class = n_samples // 2\n",
    "    theta = np.sqrt(np.random.rand(n_samples_per_class)) * turns * 2 * np.pi  # angle\n",
    "    r_a = dr * theta + noise * np.random.randn(n_samples_per_class)  # radius\n",
    "    x_a = r_a * np.cos(theta)\n",
    "    y_a = r_a * np.sin(theta)\n",
    "    class_a = np.zeros(n_samples_per_class, dtype=int)\n",
    "    r_b = -dr * theta + noise * np.random.randn(n_samples_per_class)  # radius\n",
    "    x_b = r_b * np.cos(theta)\n",
    "    y_b = r_b * np.sin(theta)\n",
    "    class_b = np.ones(n_samples_per_class, dtype=int)\n",
    "    return np.vstack([np.column_stack([x_a, y_a]), np.column_stack([x_b, y_b])]), np.hstack([class_a, class_b])\n",
    "\n",
    "X, y = make_spirals(n_samples=1000, noise=0.3, dr=0.2, random_state=0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Spirals Dataset\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# --- Configuration ---\n",
    "k_ = None         # number of neighbors - use default sqrt(n_samples)\n",
    "h_ = None         # RBF radius (std. dev.) - use default based on training set\n",
    "random_state = 0\n",
    "n_samples = 1000\n",
    "\n",
    "# Choose toy dataset\n",
    "TOY_DATASET = 'spirals'  # options: 'moons', 'circles', 'spirals'\n",
    "\n",
    "if TOY_DATASET == 'moons':\n",
    "    def make_toy(n_samples, noise=0.33):\n",
    "        return make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
    "elif TOY_DATASET == 'circles':\n",
    "    def make_toy(n_samples, noise=0.2):\n",
    "        return make_circles(n_samples=n_samples, noise=noise, factor=0.5, random_state=random_state)\n",
    "elif TOY_DATASET == 'spirals':\n",
    "    def make_toy(n_samples, noise=0.3, dr=0.2):\n",
    "        return make_spirals(n_samples=n_samples, noise=noise, dr=dr, random_state=random_state)\n",
    "else:\n",
    "    raise ValueError(\"Invalid TOY_DATASET. Choose 'moons', 'circles', 'spirals'.\")\n",
    "\n",
    "# --- Prepare data ---\n",
    "X, y = make_toy(n_samples)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.66, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "# --- Build pipeline: scaling + classifier ---\n",
    "clf_ref = RBFKNeighborsClassifier(k=k_, h=h_)  # use your implementation\n",
    "pipe = make_pipeline(StandardScaler(), clf_ref)\n",
    "\n",
    "# --- Fit and evaluate ---\n",
    "pipe.fit(X_train, y_train)\n",
    "acc = pipe.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {acc:.4f}  (k={pipe.named_steps['rbfkneighborsclassifier'].k_}, h={pipe.named_steps['rbfkneighborsclassifier'].h_})\")\n",
    "\n",
    "# --- Decision boundary visualization ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# DecisionBoundaryDisplay will plot the decision function\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    pipe,\n",
    "    X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax, #type: ignore\n",
    ")\n",
    "\n",
    "# Overlay training points (color-coded by class)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, label=\"train samples\", alpha=0.9)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='x', s=40, label=\"test samples\", alpha=0.9)\n",
    "\n",
    "ax.set_title(f\"RBF-kNN decision boundary (k={pipe.named_steps['rbfkneighborsclassifier'].k_}, h={pipe.named_steps['rbfkneighborsclassifier'].h_:.2f}) â€” test acc {acc:.3f}\")\n",
    "ax.set_xlabel(\"feature 0\")\n",
    "ax.set_ylabel(\"feature 1\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabriel_graph(X):\n",
    "    \"\"\"\n",
    "    Compute the Gabriel graph matrix for a set of points.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input points\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    adjacency_matrix : ndarray of shape (n_samples, n_samples)\n",
    "        Binary adjacency matrix where 1 indicates an edge and 0 indicates no edge\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    n_points = X.shape[0]\n",
    "    adjacency_matrix = np.zeros((n_points, n_points), dtype=int)\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        for j in range(i + 1, n_points):\n",
    "            # Midpoint of edge (i, j)\n",
    "            midpoint = (X[i] + X[j]) / 2\n",
    "            # Radius of the circle (half the distance between i and j)\n",
    "            radius = np.linalg.norm(X[i] - X[j]) / 2\n",
    "            \n",
    "            # Check if any other point lies inside the circle\n",
    "            is_gabriel_edge = True\n",
    "            for k in range(n_points):\n",
    "                if k != i and k != j:\n",
    "                    dist_to_midpoint = np.linalg.norm(X[k] - midpoint)\n",
    "                    if dist_to_midpoint < radius:\n",
    "                        is_gabriel_edge = False\n",
    "                        break\n",
    "            \n",
    "            if is_gabriel_edge:\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1\n",
    "    \n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda225d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gabriel graph for X_train\n",
    "gabriel_adj = gabriel_graph(X_train)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the points colored by class\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, label='Points', cmap='viridis')\n",
    "\n",
    "# Draw edges for Gabriel graph\n",
    "n_points = X_train.shape[0]\n",
    "for i in range(n_points):\n",
    "    for j in range(i + 1, n_points):\n",
    "        if gabriel_adj[i, j] == 1:\n",
    "            plt.plot([X_train[i, 0], X_train[j, 0]], \n",
    "                    [X_train[i, 1], X_train[j, 1]],\n",
    "                    'k', alpha=0.5)\n",
    "\n",
    "plt.title('Gabriel Graph for X_train')\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "n_edges = np.sum(gabriel_adj) // 2  # divide by 2 since matrix is symmetric\n",
    "print(f\"Number of points: {n_points}\")\n",
    "print(f\"Number of Gabriel graph edges: {n_edges}\")\n",
    "print(f\"Maximum possible edges: {n_points * (n_points - 1) // 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_graph(gabriel_adj, y):\n",
    "    \"\"\"\n",
    "    Return an adjacency matrix indicating Gabriel edges between points of different classes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gabriel_adj : array-like of shape (n_samples, n_samples)\n",
    "        Binary adjacency matrix of Gabriel graph\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Class labels for each sample\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    support_graph_matrix : ndarray of shape (n_samples, n_samples)\n",
    "        Binary matrix where 1 indicates a Gabriel edge between points of different classes\n",
    "    \"\"\"\n",
    "    gabriel_adj = np.asarray(gabriel_adj)\n",
    "    y = np.asarray(y)\n",
    "    n_samples = gabriel_adj.shape[0]\n",
    "    \n",
    "    # Create matrix indicating different classes\n",
    "    different_classes = (y[:, np.newaxis] != y[np.newaxis, :]).astype(int)\n",
    "    \n",
    "    # Element-wise multiplication: 1 only if Gabriel edge AND different classes\n",
    "    support_graph_matrix = gabriel_adj * different_classes\n",
    "\n",
    "    return support_graph_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b6944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vectors(X, y, support_graph_matrix):\n",
    "    \"\"\"\n",
    "    Identify support vectors based on the support graph matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input points\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Class labels for each sample\n",
    "    support_graph_matrix : array-like of shape (n_samples, n_samples)\n",
    "        Binary adjacency matrix indicating Gabriel edges between points of different classes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    support_vectors: ndarray of shape (m, n_features)\n",
    "        Points that are support vectors (connected to at least one point of a different class)\n",
    "    \"\"\"\n",
    "    support_graph_matrix = np.asarray(support_graph_matrix)\n",
    "    \n",
    "    # A point is a support vector if it has at least one edge in the support graph\n",
    "    is_support_vector = np.any(support_graph_matrix, axis=1)\n",
    "\n",
    "    return X[is_support_vector], y[is_support_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute support graph and support vectors for the training data\n",
    "support_graph_mat = support_graph(gabriel_adj, y_train)\n",
    "X_sup, _ = support_vectors(X_train, y_train, support_graph_mat)\n",
    "\n",
    "# Create visualization of support vectors\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot all training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, alpha=0.6, \n",
    "           label='Training points', cmap='viridis')\n",
    "\n",
    "# Highlight support vectors\n",
    "plt.scatter(X_sup[:, 0], X_sup[:, 1], \n",
    "           s=100, facecolors='none', edgecolors='black', linewidth=1,\n",
    "           label='Support vectors')\n",
    "\n",
    "plt.title('Support Vectors on Moons Dataset')\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total training points: {len(X_train)}\")\n",
    "print(f\"Number of support vectors: {len(X_sup)}\")\n",
    "print(f\"Percentage of support vectors: {len(X_sup)/len(X_train)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08013444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def gabriel_approx(X):\n",
    "    \"\"\"\n",
    "    Hyperparameter-free approximate Gabriel graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input points\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    adjacency_matrix : ndarray of shape (n_samples, n_samples)\n",
    "        Binary adjacency matrix where 1 indicates an edge and 0 indicates no edge\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    n, d = X.shape\n",
    "    \n",
    "    if n <= 1:\n",
    "        return np.zeros((n, n), dtype=int)\n",
    "\n",
    "    # Data-driven choices for number of projections and neighbor window\n",
    "    m = max(1, int(np.ceil(np.log2(n))))\n",
    "    w = max(2, int(np.ceil(np.log2(n))))\n",
    "\n",
    "    # deterministic seed derived from X\n",
    "    # create a cheap hash from rounded coordinates\n",
    "    flat = np.floor(X.ravel() * 1e6).astype(np.int64)\n",
    "    seed = int((flat.sum() ^ (flat.prod() if flat.size<100 else flat[:100].sum())) & 0xFFFFFFFF)\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # build KD-tree for radius queries\n",
    "    tree = cKDTree(X)\n",
    "\n",
    "    # prepare projection vectors\n",
    "    vs = rng.normal(size=(m, d))\n",
    "    # normalize\n",
    "    vs /= np.linalg.norm(vs, axis=1, keepdims=True)\n",
    "\n",
    "    candidates = set()\n",
    "    for v in vs:\n",
    "        s = X.dot(v)\n",
    "        order = np.argsort(s)\n",
    "        # neighbor window in 1-D ordering\n",
    "        for pos, i in enumerate(order):\n",
    "            lo = max(0, pos - w)\n",
    "            hi = min(n - 1, pos + w)\n",
    "            # enumerate neighbor indices in the ordering\n",
    "            for qpos in range(lo, hi + 1):\n",
    "                j = order[qpos]\n",
    "                if i < j:\n",
    "                    candidates.add((i, j))\n",
    "                elif j < i:\n",
    "                    candidates.add((j, i))\n",
    "\n",
    "    # epsilon for numeric robustness (data-driven)\n",
    "    max_abs = np.max(np.abs(X)) if X.size > 0 else 1.0\n",
    "    eps = 1e-12 + np.finfo(float).eps * (max_abs + 1.0)\n",
    "\n",
    "    # Initialize adjacency matrix\n",
    "    adjacency_matrix = np.zeros((n, n), dtype=int)\n",
    "    \n",
    "    # check Gabriel property for each candidate pair\n",
    "    # use KD-tree radius query\n",
    "    for (i, j) in candidates:\n",
    "        pi = X[i]\n",
    "        pj = X[j]\n",
    "        mid = (pi + pj) * 0.5\n",
    "        r = 0.5 * np.linalg.norm(pi - pj)\n",
    "        # query for any point within radius r + tiny_eps\n",
    "        idxs = tree.query_ball_point(mid, r + eps)\n",
    "        # remove i and j if present\n",
    "        has_other = any(k != i and k != j for k in idxs)\n",
    "        if not has_other:\n",
    "            adjacency_matrix[i, j] = 1\n",
    "            adjacency_matrix[j, i] = 1\n",
    "\n",
    "    return adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute approximated Gabriel graph for X_train\n",
    "gabriel_approx_adj = gabriel_approx(X_train)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the points colored by class\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, label='Points', cmap='viridis')\n",
    "\n",
    "# Draw edges for approximated Gabriel graph\n",
    "n_points = X_train.shape[0]\n",
    "for i in range(n_points):\n",
    "    for j in range(i + 1, n_points):\n",
    "        if gabriel_approx_adj[i, j] == 1:\n",
    "            plt.plot([X_train[i, 0], X_train[j, 0]], \n",
    "                    [X_train[i, 1], X_train[j, 1]],\n",
    "                    'k', alpha=0.5)\n",
    "\n",
    "plt.title('Approximated Gabriel Graph for X_train')\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "n_edges = np.sum(gabriel_approx_adj) // 2  # divide by 2 since matrix is symmetric\n",
    "print(f\"Number of points: {n_points}\")\n",
    "print(f\"Number of approximated Gabriel graph edges: {n_edges}\")\n",
    "print(f\"Maximum possible edges: {n_points * (n_points - 1) // 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute support graph and support vectors for the training data\n",
    "support_approx_mat = support_graph(gabriel_approx_adj, y_train)\n",
    "X_sup, _ = support_vectors(X_train, y_train, support_approx_mat)\n",
    "\n",
    "# Create visualization of support vectors\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot all training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, alpha=0.6, \n",
    "           label='Training points', cmap='viridis')\n",
    "\n",
    "# Highlight support vectors\n",
    "plt.scatter(X_sup[:, 0], X_sup[:, 1], \n",
    "           s=100, facecolors='none', edgecolors='black', linewidth=1,\n",
    "           label='Approximated Support vectors')\n",
    "\n",
    "plt.title('Approximated Support Vectors on Moons Dataset')\n",
    "plt.xlabel('Feature 0')\n",
    "plt.ylabel('Feature 1')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total training points: {len(X_train)}\")\n",
    "print(f\"Number of approximated support vectors: {len(X_sup)}\")\n",
    "print(f\"Percentage of approximated support vectors: {len(X_sup)/len(X_train)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fe535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRBFKNeighborsClassifier(RBFKNeighborsClassifier):\n",
    "    \"\"\"\n",
    "    k-NN classifier with RBF (isotropic multivariate normal) weighting,\n",
    "    using only support vectors from the Gabriel graph between classes.\n",
    "    \"\"\"\n",
    "\n",
    "    sv_mask: np.ndarray\n",
    "\n",
    "    def get_support_samples(self, X, y):\n",
    "        X, y = super().pre_fit(X, y)\n",
    "        gabriel_adj = gabriel_approx(X)\n",
    "        support_graph_mat = support_graph(gabriel_adj, y)\n",
    "        X_sup, y_sup = support_vectors(X, y, support_graph_mat)\n",
    "\n",
    "        self.sv_mask = np.any(support_graph_mat, axis=1)\n",
    "\n",
    "        return X_sup, y_sup\n",
    "\n",
    "    def pre_fit(self, X, y):\n",
    "        return self.get_support_samples(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c541c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0) #type: ignore\n",
    "\n",
    "clf_ref = RBFKNeighborsClassifier()\n",
    "clf_ref.fit(X_train, y_train)\n",
    "\n",
    "clf_sv = SVRBFKNeighborsClassifier()\n",
    "clf_sv.fit(X_train, y_train)\n",
    "\n",
    "ref_acc = clf_ref.score(X_test, y_test)\n",
    "sv_acc = clf_sv.score(X_test, y_test)\n",
    "\n",
    "print(f\"Reference k-NN accuracy: {ref_acc:.3f} (using {len(X_train)} points)\")\n",
    "print(f\"Support-vector k-NN accuracy: {sv_acc:.3f} (using {len(clf_sv.X_)} points)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f797e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "n_samples = 1000\n",
    "noise = 0.15\n",
    "\n",
    "# --- Prepare data ---\n",
    "X, y = make_toy(n_samples=n_samples, noise=noise)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "# --- Build reference pipeline: scaling + classifier ---\n",
    "clf_ref = RBFKNeighborsClassifier()\n",
    "pipe = make_pipeline(StandardScaler(), clf_ref)\n",
    "\n",
    "# --- Fit and evaluate reference ---\n",
    "pipe.fit(X_train, y_train)\n",
    "acc = pipe.score(X_test, y_test)\n",
    "\n",
    "# --- Build support-vector pipeline: scaling + classifier ---\n",
    "clf_sv = SVRBFKNeighborsClassifier()\n",
    "pipe_sv = make_pipeline(StandardScaler(), clf_sv)\n",
    "\n",
    "# --- Fit and evaluate support-vector ---\n",
    "pipe_sv.fit(X_train, y_train)\n",
    "sv_acc = pipe_sv.score(X_test, y_test)\n",
    "\n",
    "# --- Decision boundary visualization ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left plot: Reference classifier\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    pipe,\n",
    "    X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, label=\"train samples\", alpha=0.9)\n",
    "ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='x', s=40, label=\"test samples\", alpha=0.9)\n",
    "ax1.set_title(f\"Reference RBF-kNN (k={pipe.named_steps['rbfkneighborsclassifier'].k_}, h={pipe.named_steps['rbfkneighborsclassifier'].h_:.2f}, acc={acc:.3f})\\nUsing {len(X_train)} training samples\")\n",
    "ax1.set_xlabel(\"feature 0\")\n",
    "ax1.set_ylabel(\"feature 1\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "# Right plot: Support vector classifier\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    pipe_sv,\n",
    "    X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax2,\n",
    ")\n",
    "\n",
    "sv_mask = pipe_sv.named_steps['svrbfkneighborsclassifier'].sv_mask\n",
    "\n",
    "X_sup, y_sup = X_train[sv_mask], y_train[sv_mask]\n",
    "\n",
    "ax2.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, label=\"train samples\", alpha=0.6)\n",
    "ax2.scatter(X_sup[:, 0], X_sup[:, 1], c=y_sup, s=30, edgecolors='black', label=\"support vectors\")\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='x', s=40, label=\"test samples\", alpha=0.9)\n",
    "ax2.set_title(f\"Support-Vector RBF-kNN (k={pipe_sv.named_steps['svrbfkneighborsclassifier'].k_}, h={pipe_sv.named_steps['svrbfkneighborsclassifier'].h_:.2f}, acc={sv_acc:.3f})\\nUsing {len(X_sup)} support vectors\")\n",
    "ax2.set_xlabel(\"feature 0\")\n",
    "ax2.set_ylabel(\"feature 1\")\n",
    "ax2.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get likelihood space for both models\n",
    "likelihood_ref = clf_ref.likelihood_space()\n",
    "likelihood_sv = clf_sv.likelihood_space()\n",
    "\n",
    "# Create visualization of likelihood spaces\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left plot: Reference classifier likelihood space\n",
    "scatter1 = ax1.scatter(likelihood_ref[:, 0], likelihood_ref[:, 1], \n",
    "                      c=y_train, s=30, cmap='viridis')\n",
    "ax1.set_xlabel('P(Class 0)')\n",
    "ax1.set_ylabel('P(Class 1)')\n",
    "ax1.set_title(f'Reference RBF-kNN Likelihood Space\\n({len(X_train)} training samples)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Support vector classifier likelihood space\n",
    "scatter2 = ax2.scatter(likelihood_sv[:, 0], likelihood_sv[:, 1], \n",
    "                      c=y_train[sv_mask], s=30, cmap='viridis')\n",
    "ax2.set_xlabel('P(Class 0)')\n",
    "ax2.set_ylabel('P(Class 1)')\n",
    "ax2.set_title(f'Support-Vector RBF-kNN Likelihood Space\\n({len(X_sup)} support vectors)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbars\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_label('True Class')\n",
    "cbar2 = plt.colorbar(scatter2, ax=ax2)\n",
    "cbar2.set_label('True Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Reference model likelihood space shape: {likelihood_ref.shape}\")\n",
    "print(f\"Support vector model likelihood space shape: {likelihood_sv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22452934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from interruptingcow import timeout\n",
    "\n",
    "def plot_sweep_h(classifier_type, X, y, h_values, likelihood_space_metric, metric_name='Metric'):\n",
    "    \"\"\"\n",
    "    Create a Pipeline with scaling and the given classifier type.\n",
    "    Sweep over h values, fit the model, and plot test accuracy and likelihood space metric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier_type : type\n",
    "        The type of classifier to use\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input features\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Class labels\n",
    "    h_values : list or array-like\n",
    "        List of h values to sweep over\n",
    "    likelihood_space_metric : callable\n",
    "        Function to compute a metric from the likelihood space (e.g., variance)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    h_max_acc : float\n",
    "        h value that gives the maximum test accuracy\n",
    "    h_max_metric : float\n",
    "        h value that gives the maximum likelihood space metric\n",
    "    h_default : float\n",
    "        mean h value computed by the classifier's default method\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(shuffle=True, random_state=0)\n",
    "    \n",
    "    accuracies = []\n",
    "    metrics = []\n",
    "    h_0s = []\n",
    "    h_1s = []\n",
    "    h_2s = []\n",
    "    h_3s = []\n",
    "    \n",
    "    for h in h_values:\n",
    "        clf = classifier_type(h=h)\n",
    "        pipe = make_pipeline(StandardScaler(), clf)\n",
    "        \n",
    "        fold_accuracies = []\n",
    "        fold_metrics = []\n",
    "        \n",
    "        fold_n = 0\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X, y):\n",
    "            X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "            y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "            fold_n += 1\n",
    "            \n",
    "            try:\n",
    "                with timeout(0.5 * 60.0): # minutes * 60 = seconds\n",
    "                    n_train_samples = X_train_fold.shape[0]\n",
    "\n",
    "                    # Fit and evaluate accuracy\n",
    "                    pipe.fit(X_train_fold, y_train_fold)\n",
    "                    acc = pipe.score(X_val_fold, y_val_fold)\n",
    "                    fold_accuracies.append(acc)\n",
    "\n",
    "                    # Access the classifier step in the pipeline\n",
    "                    access_clf = pipe.named_steps[clf.__class__.__name__.lower()]\n",
    "                    \n",
    "                    # Compute likelihood space metric\n",
    "                    likelihood = access_clf.likelihood_space()\n",
    "                    metric_value = likelihood_space_metric(likelihood, access_clf.y_, h)\n",
    "                    fold_metrics.append(metric_value)\n",
    "\n",
    "                    # Store what would be the guess h's for this fold\n",
    "\n",
    "                    # h0 = exp(-1) / (mean(dist_training_samples)^-2 + mean(dist_training_samples)^-1/2)\n",
    "                    dists = np.linalg.norm(X[:, np.newaxis, :] - X[np.newaxis, :, :], axis=2)\n",
    "                    mean_dist = np.sum(dists) / (n_train_samples * (n_train_samples - 1))\n",
    "                    h_0s.append(np.exp(-1) / (mean_dist ** -2 + mean_dist ** -0.5))\n",
    "\n",
    "                    # h1 = mean distance to k-th nearest neighbor\n",
    "                    k_eff = access_clf._k_eff\n",
    "                    dists_to_kth = np.partition(dists, k_eff, axis=1)\n",
    "                    h_1s.append(np.mean(dists_to_kth[:, k_eff - 1]))\n",
    "\n",
    "                    # h2 = mean distance to nearest neighbor\n",
    "                    dists_to_1st = np.partition(dists, 1, axis=1)\n",
    "                    h_2s.append(np.mean(dists_to_1st[:, 1]))\n",
    "\n",
    "                    # h3 = smallest non-zero distance\n",
    "                    dists_no_diag = dists + np.eye(dists.shape[0]) * np.max(dists)\n",
    "                    h_3s.append(np.min(dists_no_diag))\n",
    "\n",
    "\n",
    "            except RuntimeError as e:\n",
    "\n",
    "                fold_accuracies.append(0.0)\n",
    "                fold_metrics.append(0.0)\n",
    "\n",
    "                print(f'Error {e} for h={h}, fold {fold_n}, skipping this fold.')\n",
    "\n",
    "        accuracies.append(np.mean(fold_accuracies))\n",
    "        metrics.append(np.mean(fold_metrics))\n",
    "    \n",
    "    # Find optimal h values\n",
    "    h_max_acc = h_values[np.argmax(accuracies)]\n",
    "    h_max_metric = h_values[np.argmax(metrics)]\n",
    "\n",
    "    # Compute mean of the h guesses\n",
    "    h_0 = float(np.mean(h_0s))\n",
    "    h_1 = float(np.mean(h_1s))\n",
    "    h_2 = float(np.mean(h_2s))\n",
    "    h_3 = float(np.mean(h_3s))\n",
    "\n",
    "    # Normalize metrics for plotting\n",
    "    metrics = np.array(metrics)\n",
    "    metrics = (metrics - np.min(metrics)) / (np.max(metrics) - np.min(metrics)) if np.max(metrics) > np.min(metrics) else metrics\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(h_values, accuracies, label='Test Accuracy')\n",
    "    plt.plot(h_values, metrics, label=metric_name)\n",
    "    plt.axvline(h_max_acc, color='blue', linestyle='--', label=f'Optimal h (Acc): {h_max_acc:.2f}')\n",
    "    plt.axvline(h_max_metric, color='orange', linestyle='--', label=f'Optimal h (Metric): {h_max_metric:.2f}')\n",
    "    plt.axvline(h_0, color='green', linestyle='--', label=f'h_0: {h_0:.2f}')\n",
    "    plt.axvline(h_1, color='purple', linestyle='--', label=f'h_1: {h_1:.2f}')\n",
    "    plt.axvline(h_2, color='brown', linestyle='--', label=f'h_2: {h_2:.2f}')\n",
    "    plt.axvline(h_3, color='pink', linestyle='--', label=f'h_3: {h_3:.2f}')\n",
    "    plt.xlabel('h value')\n",
    "    plt.ylabel('Score / Metric (normalized)')\n",
    "    plt.title(f'Sweep of h values for {classifier_type.__name__}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return h_max_acc, h_max_metric, h_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_likelihood_metric(metric, h_values = None, metric_name='Metric'):\n",
    "\n",
    "    # Prepare h values\n",
    "    if h_values is None:\n",
    "        h_values = np.linspace(0.01, 1.1, 100)\n",
    "\n",
    "    # Sweep for Reference RBF-kNN\n",
    "    print(\"Sweeping h values for Reference RBF-kNN...\")\n",
    "    h_max_acc_ref, h_max_metric_ref, h_default_ref = plot_sweep_h(\n",
    "        RBFKNeighborsClassifier, X, y, h_values, metric, metric_name=metric_name\n",
    "    )\n",
    "\n",
    "    # Sweep for Support Vector RBF-kNN\n",
    "    print(\"\\nSweeping h values for Support Vector RBF-kNN...\")\n",
    "    h_max_acc_sv, h_max_metric_sv, h_default_sv = plot_sweep_h(\n",
    "        SVRBFKNeighborsClassifier, X, y, h_values, metric, metric_name=metric_name\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults Summary:\")\n",
    "    print(f\"Reference RBF-kNN:\")\n",
    "    print(f\"  h for max accuracy: {h_max_acc_ref:.3f}\")\n",
    "    print(f\"  h for max avg centroid distance: {h_max_metric_ref:.3f}\")\n",
    "    print(f\"  default h: {h_default_ref:.3f}\")\n",
    "    print(f\"\\nSupport Vector RBF-kNN:\")\n",
    "    print(f\"  h for max accuracy: {h_max_acc_sv:.3f}\")\n",
    "    print(f\"  h for max avg centroid distance: {h_max_metric_sv:.3f}\")\n",
    "    print(f\"  h_0: {h_default_sv:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_distance(Q, y):\n",
    "    \"\"\"\n",
    "    Compute the distance between each pair of centroids in the likelihood space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array-like of shape (n_samples, n_classes)\n",
    "        Probabilities of each sample to belong to each class.\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Class label for each sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dm : ndarray of shape (n_classes, n_classes)\n",
    "        Distance matrix where Dm[i, j] is the Euclidean distance between centroid of class i and class j.\n",
    "    dist_avg : float\n",
    "        Average of the pairwise centroid distances (off-diagonal).\n",
    "    dist_std : float\n",
    "        Standard deviation of the pairwise centroid distances (off-diagonal).\n",
    "    \"\"\"\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    classes = np.unique(y)\n",
    "    n_classes = classes.size\n",
    "\n",
    "    # compute centroid for each class in likelihood space\n",
    "    centroids = np.zeros((n_classes, Q.shape[1]), dtype=float)\n",
    "    for idx, cls in enumerate(classes):\n",
    "        members = Q[y == cls]\n",
    "        if members.size == 0:\n",
    "            centroids[idx] = 0.0\n",
    "        else:\n",
    "            centroids[idx] = members.mean(axis=0)\n",
    "\n",
    "    # pairwise Euclidean distances between centroids\n",
    "    diff = centroids[:, np.newaxis, :] - centroids[np.newaxis, :, :]\n",
    "    Dm = np.linalg.norm(diff, axis=2)\n",
    "\n",
    "    # extract upper-triangular (off-diagonal) distances\n",
    "    if n_classes > 1:\n",
    "        triu_idx = np.triu_indices(n_classes, k=1)\n",
    "        pairwise = Dm[triu_idx]\n",
    "        dist_avg = float(np.mean(pairwise))\n",
    "        dist_std = float(np.std(pairwise))\n",
    "    else:\n",
    "        dist_avg = 0.0\n",
    "        dist_std = 0.0\n",
    "\n",
    "    return Dm, dist_avg, dist_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_centroid_distance(Q, y, _):\n",
    "    \"\"\"Compute the average centroid distance in likelihood space\"\"\"\n",
    "    _, dist_avg, _ = centroid_distance(Q, y)\n",
    "    return dist_avg\n",
    "\n",
    "test_likelihood_metric(avg_centroid_distance, metric_name='Average Centroid Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b877bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_centroid_distance_weighted(Q, y, h):\n",
    "    \"\"\"Compute the average centroid distance in likelihood space\"\"\"\n",
    "    _, dist_avg, _ = centroid_distance(Q, y)\n",
    "    return -dist_avg * h\n",
    "\n",
    "test_likelihood_metric(avg_centroid_distance_weighted, metric_name='Average Centroid Distance Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1154ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_spread(Q, y):\n",
    "    \"\"\"\n",
    "    Compute the pairwise distance between samples in the likelihood space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array-like of shape (n_samples, n_classes)\n",
    "        Probabilities of each sample to belong to each class.\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Class label for each sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    inter_class_avg : float\n",
    "        Average pairwise distance between samples of different classes.\n",
    "    intra_class_avg : float\n",
    "        Average pairwise distance between samples of the same class.\n",
    "    inter_class_std : float\n",
    "        Standard deviation of pairwise distances between samples of different classes.\n",
    "    intra_class_std : float\n",
    "        Standard deviation of pairwise distances between samples of the same class.\n",
    "    inter_class_min : float\n",
    "        Minimum pairwise distance between samples of different classes.\n",
    "    intra_class_min : float\n",
    "        Minimum pairwise distance between samples of the same class.\n",
    "    \"\"\"\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    n_samples = Q.shape[0]\n",
    "    if n_samples <= 1:\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # pairwise Euclidean distances between samples\n",
    "    diff = Q[:, np.newaxis, :] - Q[np.newaxis, :, :]\n",
    "    D = np.linalg.norm(diff, axis=2)\n",
    "\n",
    "    inter_class_dists = []\n",
    "    intra_class_dists = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(i + 1, n_samples):\n",
    "            if y[i] == y[j]:\n",
    "                intra_class_dists.append(D[i, j])\n",
    "            else:\n",
    "                inter_class_dists.append(D[i, j])\n",
    "\n",
    "    inter_class_avg = float(np.mean(inter_class_dists)) if inter_class_dists else 0.0\n",
    "    intra_class_avg = float(np.mean(intra_class_dists)) if intra_class_dists else 0.0\n",
    "    inter_class_std = float(np.std(inter_class_dists)) if inter_class_dists else 0.0\n",
    "    intra_class_std = float(np.std(intra_class_dists)) if intra_class_dists else 0.0\n",
    "    inter_class_min = float(np.min(inter_class_dists)) if inter_class_dists else 0.0\n",
    "    intra_class_min = float(np.min(intra_class_dists)) if intra_class_dists else 0.0\n",
    "\n",
    "    return inter_class_avg, intra_class_avg, inter_class_std, intra_class_std, inter_class_min, intra_class_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6fbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_class_distance(Q, y):\n",
    "    \"\"\"Compute the average inter-class distance in likelihood space\"\"\"\n",
    "    inter_avg, _, inter_std, _, _, _ = sample_spread(Q, y)\n",
    "    return inter_avg - inter_std\n",
    "\n",
    "test_likelihood_metric(inter_class_distance, metric_name='Inter-Class Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_class_distance_weighted(Q, y, h):\n",
    "    \"\"\"Compute the average inter-class distance in likelihood space\"\"\"\n",
    "    inter_avg, _, inter_std, _, _, _ = sample_spread(Q, y)\n",
    "    return - (inter_avg - inter_std) * h\n",
    "\n",
    "test_likelihood_metric(inter_class_distance_weighted, metric_name='Inter-Class Distance Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7724ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intra_class_distance(Q, y):\n",
    "    \"\"\"Compute the average intra-class distance in likelihood space\"\"\"\n",
    "    _, intra_avg, _, intra_std, _, _ = sample_spread(Q, y)\n",
    "    return intra_avg - intra_std\n",
    "\n",
    "test_likelihood_metric(intra_class_distance, metric_name='Intra-Class Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a128d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_distance(Q, y):\n",
    "    \"\"\"Compute the overall average pairwise distance in likelihood space\"\"\"\n",
    "    inter_avg, intra_avg, inter_std, intra_std, _, _ = sample_spread(Q, y)\n",
    "    return inter_avg + intra_avg - inter_std - intra_std\n",
    "\n",
    "test_likelihood_metric(combined_distance, metric_name='Combined Sample Distances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33038d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_distance_weighted(Q, y, h):\n",
    "    \"\"\"Compute the overall average pairwise distance in likelihood space\"\"\"\n",
    "    inter_avg, intra_avg, inter_std, intra_std, _, _ = sample_spread(Q, y)\n",
    "    return - (inter_avg + intra_avg - inter_std - intra_std) * h\n",
    "\n",
    "test_likelihood_metric(combined_distance_weighted, metric_name='Combined Sample Distances Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sample_distance(Q, y):\n",
    "    \"\"\"Compute the minimum pairwise distance in likelihood space\"\"\"\n",
    "    _, _, _, _, inter_min, intra_min = sample_spread(Q, y)\n",
    "    return inter_min + intra_min\n",
    "\n",
    "test_likelihood_metric(min_sample_distance, metric_name='Minimum Distance Between Samples*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c248d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_sample_distance_weighted(Q, y, h):\n",
    "    \"\"\"Compute the minimum pairwise distance in likelihood space\"\"\"\n",
    "    _, _, _, _, inter_min, intra_min = sample_spread(Q, y)\n",
    "    return - (inter_min + intra_min) * h\n",
    "\n",
    "test_likelihood_metric(min_sample_distance_weighted, metric_name='Minimum Distance Between Samples Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya(Q, y):\n",
    "    \"\"\"\n",
    "    Compute the Bhattacharyya distance between class distributions in likelihood space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array-like of shape (n_samples, n_classes)\n",
    "        Probabilities of each sample to belong to each class.\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Class label for each sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Davg : float\n",
    "        Average Bhattacharyya distance between all pairs of class distributions.\n",
    "    Dstd : float\n",
    "        Standard deviation of the Bhattacharyya distances.\n",
    "    Dmin : float\n",
    "        Minimum Bhattacharyya distance between any pair of class distributions.\n",
    "    Dmax : float\n",
    "        Maximum Bhattacharyya distance between any pair of class distributions.\n",
    "    \"\"\"\n",
    "    Q = np.asarray(Q, dtype=float)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    classes = np.unique(y)\n",
    "    n_classes = classes.size\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # compute mean and covariance for each class in likelihood space\n",
    "    means = []\n",
    "    covs = []\n",
    "    for cls in classes:\n",
    "        members = Q[y == cls]\n",
    "        if members.shape[0] < 2:\n",
    "            # not enough members to compute covariance\n",
    "            means.append(members.mean(axis=0))\n",
    "            covs.append(np.eye(Q.shape[1]) * 1e-6)  # tiny covariance\n",
    "        else:\n",
    "            means.append(members.mean(axis=0))\n",
    "            covs.append(np.cov(members, rowvar=False) + np.eye(Q.shape[1]) * 1e-6)  # regularize\n",
    "\n",
    "    means = np.array(means)\n",
    "    covs = np.array(covs)\n",
    "\n",
    "    dists = []\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i + 1, n_classes):\n",
    "            mu1 = means[i]\n",
    "            mu2 = means[j]\n",
    "            S1 = covs[i]\n",
    "            S2 = covs[j]\n",
    "\n",
    "            # Bhattacharyya distance formula\n",
    "            S_avg = (S1 + S2) / 2\n",
    "            try:\n",
    "                term1 = 0.125 * (mu1 - mu2).T @ np.linalg.inv(S_avg) @ (mu1 - mu2)\n",
    "                term2 = 0.5 * np.log(np.linalg.det(S_avg) / np.sqrt(np.linalg.det(S1) * np.linalg.det(S2)))\n",
    "                dist = term1 + term2\n",
    "                if np.isfinite(dist):\n",
    "                    dists.append(dist)\n",
    "            except np.linalg.LinAlgError:\n",
    "                continue\n",
    "\n",
    "    if dists:\n",
    "        Davg = float(np.mean(dists))\n",
    "        Dstd = float(np.std(dists))\n",
    "        Dmin = float(np.min(dists))\n",
    "        Dmax = float(np.max(dists))\n",
    "    else:\n",
    "        Davg = Dstd = Dmin = Dmax = 0.0\n",
    "\n",
    "    return Davg, Dstd, Dmin, Dmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_overall(Q, y):\n",
    "    \"\"\"Compute the overall Bhattacharyya distance metric\"\"\"\n",
    "    Davg, Dstd, _, _ = bhattacharyya(Q, y)\n",
    "    return -(Davg - Dstd)\n",
    "\n",
    "test_likelihood_metric(bhattacharyya_overall, metric_name='Bhattacharyya Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2be865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_overall_weighted(Q, y, h):\n",
    "    \"\"\"Compute the overall Bhattacharyya distance metric\"\"\"\n",
    "    Davg, Dstd, _, _ = bhattacharyya(Q, y)\n",
    "    return -(Davg - Dstd) * h\n",
    "\n",
    "test_likelihood_metric(bhattacharyya_overall_weighted, metric_name='Bhattacharyya Distance Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_min(Q, y):\n",
    "    \"\"\"Compute the minimum Bhattacharyya distance metric\"\"\"\n",
    "    _, _, Dmin, _ = bhattacharyya(Q, y)\n",
    "    return -Dmin\n",
    "\n",
    "test_likelihood_metric(bhattacharyya_min, metric_name='Minimum Bhattacharyya Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_min_weighted(Q, y, h):\n",
    "    \"\"\"Compute the minimum Bhattacharyya distance metric\"\"\"\n",
    "    _, _, Dmin, _ = bhattacharyya(Q, y)\n",
    "    return -Dmin * h\n",
    "\n",
    "test_likelihood_metric(bhattacharyya_min_weighted, metric_name='Minimum Bhattacharyya Distance Weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1059497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_max(Q, y):\n",
    "    \"\"\"Compute the maximum Bhattacharyya distance metric\"\"\"\n",
    "    _, _, _, Dmax = bhattacharyya(Q, y)\n",
    "    return -Dmax\n",
    "\n",
    "test_likelihood_metric(bhattacharyya_max, metric_name='Maximum Bhattacharyya Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5da025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhattacharyya_range(Q, y):\n",
    "    \"\"\"Compute the range of Bhattacharyya distances\"\"\"\n",
    "    Davg, Dstd, Dmin, Dmax = bhattacharyya(Q, y)\n",
    "    if Dstd == 0.0:\n",
    "        return Davg\n",
    "    else:\n",
    "        return -(Dmax - Dmin)\n",
    "\n",
    "test_likelihood_metric(bhattacharyya_range, metric_name='Range of Bhattacharyya Distances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545bd21",
   "metadata": {},
   "source": [
    "# Good candidates\n",
    "\n",
    "| Metric | KNN | SVKNN |\n",
    "|---|---|---|\n",
    "| Average Centroid Distance | âœ“ | x |\n",
    "| Average Centroid Distance Weighted |  |  |\n",
    "| Inter-Class Distance |  |  |\n",
    "| Inter-Class Distance Weighted |  |  |\n",
    "| Intra-Class Distance |  |  |\n",
    "| Combined Sample Distances |  |  |\n",
    "| Combined Sample Distances Weighted |  |  |\n",
    "| Minimum Distance Between Samples |  |  |\n",
    "| Minimum Distance Between Samples Weighted |  |  |\n",
    "| Bhattacharyya Distance |  |  |\n",
    "| Bhattacharyya Distance Weighted |  |  |\n",
    "| Minimum Bhattacharyya Distance |  |  |\n",
    "| Minimum Bhattacharyya Distance Weighted |  |  |\n",
    "| Maximum Bhattacharyya Distance |  |  |\n",
    "| Range of Bhattacharyya Distances |  |  |\n",
    "| h_0 |  |  |\n",
    "| h_1 |  |  |\n",
    "| h_2 |  |  |\n",
    "| h_3 |  |  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
